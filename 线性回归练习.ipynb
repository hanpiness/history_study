{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_data = np.linspace(0,100,500)\n",
    "\n",
    "y_data = x_data * 3.1234 + 2.98 + np.random.randn(*x_data.shape) * 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(x,w,b):\n",
    "    return tf.multiply(x,w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = tf.Variable(1.0,tf.float32)\n",
    "b = tf.Variable(0.0,tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(x,y,w,b):\n",
    "    err = model(x,w,b) - y   #计算模型预测值和标签值的差异\n",
    "    squared_err = tf.square(err)  #求平方，得出方差\n",
    "    return tf.reduce_mean(squared_err)  #求均值,得出均方差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_epochs = 10  # 迭代次数(训练轮数)\n",
    "learning_rate = 0.0001  # 学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad(x,y,w,b):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_ = loss(x,y,w,b)\n",
    "    return tape.gradient(loss_,[w,b])   # 返回梯度向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 01 Step: 020 loss_nan\n",
      "Training Epoch: 01 Step: 040 loss_nan\n",
      "Training Epoch: 01 Step: 060 loss_nan\n",
      "Training Epoch: 01 Step: 080 loss_nan\n",
      "Training Epoch: 01 Step: 100 loss_nan\n",
      "Training Epoch: 01 Step: 120 loss_nan\n",
      "Training Epoch: 01 Step: 140 loss_nan\n",
      "Training Epoch: 01 Step: 160 loss_nan\n",
      "Training Epoch: 01 Step: 180 loss_nan\n",
      "Training Epoch: 01 Step: 200 loss_nan\n",
      "Training Epoch: 01 Step: 220 loss_nan\n",
      "Training Epoch: 01 Step: 240 loss_nan\n",
      "Training Epoch: 01 Step: 260 loss_nan\n",
      "Training Epoch: 01 Step: 280 loss_nan\n",
      "Training Epoch: 01 Step: 300 loss_nan\n",
      "Training Epoch: 01 Step: 320 loss_nan\n",
      "Training Epoch: 01 Step: 340 loss_nan\n",
      "Training Epoch: 01 Step: 360 loss_nan\n",
      "Training Epoch: 01 Step: 380 loss_nan\n",
      "Training Epoch: 01 Step: 400 loss_nan\n",
      "Training Epoch: 01 Step: 420 loss_nan\n",
      "Training Epoch: 01 Step: 440 loss_nan\n",
      "Training Epoch: 01 Step: 460 loss_nan\n",
      "Training Epoch: 01 Step: 480 loss_nan\n",
      "Training Epoch: 01 Step: 500 loss_nan\n",
      "Training Epoch: 02 Step: 520 loss_nan\n",
      "Training Epoch: 02 Step: 540 loss_nan\n",
      "Training Epoch: 02 Step: 560 loss_nan\n",
      "Training Epoch: 02 Step: 580 loss_nan\n",
      "Training Epoch: 02 Step: 600 loss_nan\n",
      "Training Epoch: 02 Step: 620 loss_nan\n",
      "Training Epoch: 02 Step: 640 loss_nan\n",
      "Training Epoch: 02 Step: 660 loss_nan\n",
      "Training Epoch: 02 Step: 680 loss_nan\n",
      "Training Epoch: 02 Step: 700 loss_nan\n",
      "Training Epoch: 02 Step: 720 loss_nan\n",
      "Training Epoch: 02 Step: 740 loss_nan\n",
      "Training Epoch: 02 Step: 760 loss_nan\n",
      "Training Epoch: 02 Step: 780 loss_nan\n",
      "Training Epoch: 02 Step: 800 loss_nan\n",
      "Training Epoch: 02 Step: 820 loss_nan\n",
      "Training Epoch: 02 Step: 840 loss_nan\n",
      "Training Epoch: 02 Step: 860 loss_nan\n",
      "Training Epoch: 02 Step: 880 loss_nan\n",
      "Training Epoch: 02 Step: 900 loss_nan\n",
      "Training Epoch: 02 Step: 920 loss_nan\n",
      "Training Epoch: 02 Step: 940 loss_nan\n",
      "Training Epoch: 02 Step: 960 loss_nan\n",
      "Training Epoch: 02 Step: 980 loss_nan\n",
      "Training Epoch: 02 Step: 1000 loss_nan\n",
      "Training Epoch: 03 Step: 1020 loss_nan\n",
      "Training Epoch: 03 Step: 1040 loss_nan\n",
      "Training Epoch: 03 Step: 1060 loss_nan\n",
      "Training Epoch: 03 Step: 1080 loss_nan\n",
      "Training Epoch: 03 Step: 1100 loss_nan\n",
      "Training Epoch: 03 Step: 1120 loss_nan\n",
      "Training Epoch: 03 Step: 1140 loss_nan\n",
      "Training Epoch: 03 Step: 1160 loss_nan\n",
      "Training Epoch: 03 Step: 1180 loss_nan\n",
      "Training Epoch: 03 Step: 1200 loss_nan\n",
      "Training Epoch: 03 Step: 1220 loss_nan\n",
      "Training Epoch: 03 Step: 1240 loss_nan\n",
      "Training Epoch: 03 Step: 1260 loss_nan\n",
      "Training Epoch: 03 Step: 1280 loss_nan\n",
      "Training Epoch: 03 Step: 1300 loss_nan\n",
      "Training Epoch: 03 Step: 1320 loss_nan\n",
      "Training Epoch: 03 Step: 1340 loss_nan\n",
      "Training Epoch: 03 Step: 1360 loss_nan\n",
      "Training Epoch: 03 Step: 1380 loss_nan\n",
      "Training Epoch: 03 Step: 1400 loss_nan\n",
      "Training Epoch: 03 Step: 1420 loss_nan\n",
      "Training Epoch: 03 Step: 1440 loss_nan\n",
      "Training Epoch: 03 Step: 1460 loss_nan\n",
      "Training Epoch: 03 Step: 1480 loss_nan\n",
      "Training Epoch: 03 Step: 1500 loss_nan\n",
      "Training Epoch: 04 Step: 1520 loss_nan\n",
      "Training Epoch: 04 Step: 1540 loss_nan\n",
      "Training Epoch: 04 Step: 1560 loss_nan\n",
      "Training Epoch: 04 Step: 1580 loss_nan\n",
      "Training Epoch: 04 Step: 1600 loss_nan\n",
      "Training Epoch: 04 Step: 1620 loss_nan\n",
      "Training Epoch: 04 Step: 1640 loss_nan\n",
      "Training Epoch: 04 Step: 1660 loss_nan\n",
      "Training Epoch: 04 Step: 1680 loss_nan\n",
      "Training Epoch: 04 Step: 1700 loss_nan\n",
      "Training Epoch: 04 Step: 1720 loss_nan\n",
      "Training Epoch: 04 Step: 1740 loss_nan\n",
      "Training Epoch: 04 Step: 1760 loss_nan\n",
      "Training Epoch: 04 Step: 1780 loss_nan\n",
      "Training Epoch: 04 Step: 1800 loss_nan\n",
      "Training Epoch: 04 Step: 1820 loss_nan\n",
      "Training Epoch: 04 Step: 1840 loss_nan\n",
      "Training Epoch: 04 Step: 1860 loss_nan\n",
      "Training Epoch: 04 Step: 1880 loss_nan\n",
      "Training Epoch: 04 Step: 1900 loss_nan\n",
      "Training Epoch: 04 Step: 1920 loss_nan\n",
      "Training Epoch: 04 Step: 1940 loss_nan\n",
      "Training Epoch: 04 Step: 1960 loss_nan\n",
      "Training Epoch: 04 Step: 1980 loss_nan\n",
      "Training Epoch: 04 Step: 2000 loss_nan\n",
      "Training Epoch: 05 Step: 2020 loss_nan\n",
      "Training Epoch: 05 Step: 2040 loss_nan\n",
      "Training Epoch: 05 Step: 2060 loss_nan\n",
      "Training Epoch: 05 Step: 2080 loss_nan\n",
      "Training Epoch: 05 Step: 2100 loss_nan\n",
      "Training Epoch: 05 Step: 2120 loss_nan\n",
      "Training Epoch: 05 Step: 2140 loss_nan\n",
      "Training Epoch: 05 Step: 2160 loss_nan\n",
      "Training Epoch: 05 Step: 2180 loss_nan\n",
      "Training Epoch: 05 Step: 2200 loss_nan\n",
      "Training Epoch: 05 Step: 2220 loss_nan\n",
      "Training Epoch: 05 Step: 2240 loss_nan\n",
      "Training Epoch: 05 Step: 2260 loss_nan\n",
      "Training Epoch: 05 Step: 2280 loss_nan\n",
      "Training Epoch: 05 Step: 2300 loss_nan\n",
      "Training Epoch: 05 Step: 2320 loss_nan\n",
      "Training Epoch: 05 Step: 2340 loss_nan\n",
      "Training Epoch: 05 Step: 2360 loss_nan\n",
      "Training Epoch: 05 Step: 2380 loss_nan\n",
      "Training Epoch: 05 Step: 2400 loss_nan\n",
      "Training Epoch: 05 Step: 2420 loss_nan\n",
      "Training Epoch: 05 Step: 2440 loss_nan\n",
      "Training Epoch: 05 Step: 2460 loss_nan\n",
      "Training Epoch: 05 Step: 2480 loss_nan\n",
      "Training Epoch: 05 Step: 2500 loss_nan\n",
      "Training Epoch: 06 Step: 2520 loss_nan\n",
      "Training Epoch: 06 Step: 2540 loss_nan\n",
      "Training Epoch: 06 Step: 2560 loss_nan\n",
      "Training Epoch: 06 Step: 2580 loss_nan\n",
      "Training Epoch: 06 Step: 2600 loss_nan\n",
      "Training Epoch: 06 Step: 2620 loss_nan\n",
      "Training Epoch: 06 Step: 2640 loss_nan\n",
      "Training Epoch: 06 Step: 2660 loss_nan\n",
      "Training Epoch: 06 Step: 2680 loss_nan\n",
      "Training Epoch: 06 Step: 2700 loss_nan\n",
      "Training Epoch: 06 Step: 2720 loss_nan\n",
      "Training Epoch: 06 Step: 2740 loss_nan\n",
      "Training Epoch: 06 Step: 2760 loss_nan\n",
      "Training Epoch: 06 Step: 2780 loss_nan\n",
      "Training Epoch: 06 Step: 2800 loss_nan\n",
      "Training Epoch: 06 Step: 2820 loss_nan\n",
      "Training Epoch: 06 Step: 2840 loss_nan\n",
      "Training Epoch: 06 Step: 2860 loss_nan\n",
      "Training Epoch: 06 Step: 2880 loss_nan\n",
      "Training Epoch: 06 Step: 2900 loss_nan\n",
      "Training Epoch: 06 Step: 2920 loss_nan\n",
      "Training Epoch: 06 Step: 2940 loss_nan\n",
      "Training Epoch: 06 Step: 2960 loss_nan\n",
      "Training Epoch: 06 Step: 2980 loss_nan\n",
      "Training Epoch: 06 Step: 3000 loss_nan\n",
      "Training Epoch: 07 Step: 3020 loss_nan\n",
      "Training Epoch: 07 Step: 3040 loss_nan\n",
      "Training Epoch: 07 Step: 3060 loss_nan\n",
      "Training Epoch: 07 Step: 3080 loss_nan\n",
      "Training Epoch: 07 Step: 3100 loss_nan\n",
      "Training Epoch: 07 Step: 3120 loss_nan\n",
      "Training Epoch: 07 Step: 3140 loss_nan\n",
      "Training Epoch: 07 Step: 3160 loss_nan\n",
      "Training Epoch: 07 Step: 3180 loss_nan\n",
      "Training Epoch: 07 Step: 3200 loss_nan\n",
      "Training Epoch: 07 Step: 3220 loss_nan\n",
      "Training Epoch: 07 Step: 3240 loss_nan\n",
      "Training Epoch: 07 Step: 3260 loss_nan\n",
      "Training Epoch: 07 Step: 3280 loss_nan\n",
      "Training Epoch: 07 Step: 3300 loss_nan\n",
      "Training Epoch: 07 Step: 3320 loss_nan\n",
      "Training Epoch: 07 Step: 3340 loss_nan\n",
      "Training Epoch: 07 Step: 3360 loss_nan\n",
      "Training Epoch: 07 Step: 3380 loss_nan\n",
      "Training Epoch: 07 Step: 3400 loss_nan\n",
      "Training Epoch: 07 Step: 3420 loss_nan\n",
      "Training Epoch: 07 Step: 3440 loss_nan\n",
      "Training Epoch: 07 Step: 3460 loss_nan\n",
      "Training Epoch: 07 Step: 3480 loss_nan\n",
      "Training Epoch: 07 Step: 3500 loss_nan\n",
      "Training Epoch: 08 Step: 3520 loss_nan\n",
      "Training Epoch: 08 Step: 3540 loss_nan\n",
      "Training Epoch: 08 Step: 3560 loss_nan\n",
      "Training Epoch: 08 Step: 3580 loss_nan\n",
      "Training Epoch: 08 Step: 3600 loss_nan\n",
      "Training Epoch: 08 Step: 3620 loss_nan\n",
      "Training Epoch: 08 Step: 3640 loss_nan\n",
      "Training Epoch: 08 Step: 3660 loss_nan\n",
      "Training Epoch: 08 Step: 3680 loss_nan\n",
      "Training Epoch: 08 Step: 3700 loss_nan\n",
      "Training Epoch: 08 Step: 3720 loss_nan\n",
      "Training Epoch: 08 Step: 3740 loss_nan\n",
      "Training Epoch: 08 Step: 3760 loss_nan\n",
      "Training Epoch: 08 Step: 3780 loss_nan\n",
      "Training Epoch: 08 Step: 3800 loss_nan\n",
      "Training Epoch: 08 Step: 3820 loss_nan\n",
      "Training Epoch: 08 Step: 3840 loss_nan\n",
      "Training Epoch: 08 Step: 3860 loss_nan\n",
      "Training Epoch: 08 Step: 3880 loss_nan\n",
      "Training Epoch: 08 Step: 3900 loss_nan\n",
      "Training Epoch: 08 Step: 3920 loss_nan\n",
      "Training Epoch: 08 Step: 3940 loss_nan\n",
      "Training Epoch: 08 Step: 3960 loss_nan\n",
      "Training Epoch: 08 Step: 3980 loss_nan\n",
      "Training Epoch: 08 Step: 4000 loss_nan\n",
      "Training Epoch: 09 Step: 4020 loss_nan\n",
      "Training Epoch: 09 Step: 4040 loss_nan\n",
      "Training Epoch: 09 Step: 4060 loss_nan\n",
      "Training Epoch: 09 Step: 4080 loss_nan\n",
      "Training Epoch: 09 Step: 4100 loss_nan\n",
      "Training Epoch: 09 Step: 4120 loss_nan\n",
      "Training Epoch: 09 Step: 4140 loss_nan\n",
      "Training Epoch: 09 Step: 4160 loss_nan\n",
      "Training Epoch: 09 Step: 4180 loss_nan\n",
      "Training Epoch: 09 Step: 4200 loss_nan\n",
      "Training Epoch: 09 Step: 4220 loss_nan\n",
      "Training Epoch: 09 Step: 4240 loss_nan\n",
      "Training Epoch: 09 Step: 4260 loss_nan\n",
      "Training Epoch: 09 Step: 4280 loss_nan\n",
      "Training Epoch: 09 Step: 4300 loss_nan\n",
      "Training Epoch: 09 Step: 4320 loss_nan\n",
      "Training Epoch: 09 Step: 4340 loss_nan\n",
      "Training Epoch: 09 Step: 4360 loss_nan\n",
      "Training Epoch: 09 Step: 4380 loss_nan\n",
      "Training Epoch: 09 Step: 4400 loss_nan\n",
      "Training Epoch: 09 Step: 4420 loss_nan\n",
      "Training Epoch: 09 Step: 4440 loss_nan\n",
      "Training Epoch: 09 Step: 4460 loss_nan\n",
      "Training Epoch: 09 Step: 4480 loss_nan\n",
      "Training Epoch: 09 Step: 4500 loss_nan\n",
      "Training Epoch: 10 Step: 4520 loss_nan\n",
      "Training Epoch: 10 Step: 4540 loss_nan\n",
      "Training Epoch: 10 Step: 4560 loss_nan\n",
      "Training Epoch: 10 Step: 4580 loss_nan\n",
      "Training Epoch: 10 Step: 4600 loss_nan\n",
      "Training Epoch: 10 Step: 4620 loss_nan\n",
      "Training Epoch: 10 Step: 4640 loss_nan\n",
      "Training Epoch: 10 Step: 4660 loss_nan\n",
      "Training Epoch: 10 Step: 4680 loss_nan\n",
      "Training Epoch: 10 Step: 4700 loss_nan\n",
      "Training Epoch: 10 Step: 4720 loss_nan\n",
      "Training Epoch: 10 Step: 4740 loss_nan\n",
      "Training Epoch: 10 Step: 4760 loss_nan\n",
      "Training Epoch: 10 Step: 4780 loss_nan\n",
      "Training Epoch: 10 Step: 4800 loss_nan\n",
      "Training Epoch: 10 Step: 4820 loss_nan\n",
      "Training Epoch: 10 Step: 4840 loss_nan\n",
      "Training Epoch: 10 Step: 4860 loss_nan\n",
      "Training Epoch: 10 Step: 4880 loss_nan\n",
      "Training Epoch: 10 Step: 4900 loss_nan\n",
      "Training Epoch: 10 Step: 4920 loss_nan\n",
      "Training Epoch: 10 Step: 4940 loss_nan\n",
      "Training Epoch: 10 Step: 4960 loss_nan\n",
      "Training Epoch: 10 Step: 4980 loss_nan\n",
      "Training Epoch: 10 Step: 5000 loss_nan\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "step = 0  #记录训练的步数\n",
    "loss_list = []  #用于保存loss值的列表\n",
    "display_step = 20 # 控制训练过程数据显示的频率，不是超参数\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    for xs,ys in zip(x_data,y_data):\n",
    "        \n",
    "        loss_ = loss(xs,ys,w, b)   #计算损失\n",
    "        loss_list.append(loss_)    #保存本次损失计算结果\n",
    "        \n",
    "        delta_w,delta_b = grad(xs,ys,w,b)   #计算当前[w,b]点的梯度\n",
    "        change_w = delta_w * learning_rate  #计算变量w需要改变的量\n",
    "        change_b = delta_b * learning_rate  #计算变量b需要改变的量\n",
    "        w.assign_sub(change_w)              #变量w值变更为减去change_w后的值\n",
    "        b.assign_sub(change_b)              #变量b值变更为减去change_w后的值\n",
    "        \n",
    "        step = step+1                       #训练步数加一\n",
    "        if step % display_step==0:          #显示训练过程\n",
    "            print(\"Training Epoch:\",'%02d'%(epoch+1),\"Step: %03d\"%(step),\"loss_%.f\"%(loss_))\n",
    "    plt.plot(x_data,w.numpy() * x_data + b.numpy())    # 完成一轮训练后，画出图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "env-tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
